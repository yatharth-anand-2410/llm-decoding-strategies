{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logits and Greedy Decoding\n",
    "\n",
    "In this notebook, we'll strip away the magic of `model.generate()` and look at exactly what an LLM outputs given a prompt. Then, we will implement the simplest text generation strategy: **Greedy Decoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4df28cd0a634747805c10d571f06b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-0.5B Model and tokenizer loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set up device (MPS is for Apple Silicon Macs, CUDA for Nvidia, otherwise CPU)\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# We'll use Qwen/Qwen2.5-0.5B for this default as it is fast and easy to understand.\n",
    "# Other great tiny models you can swap here:\n",
    "# - \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "# - \"gpt 2\"\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "model.eval()  # Make sure we're in eval mode (disables dropout, etc.)\n",
    "print(f\"{model_id} Model and tokenizer loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: The Forward Pass and Logits\n",
    "When you pass tokens into the model, it outputs `logits`. These are raw, un-normalized scores for every single token in the vocabulary, representing how strongly the model believes that token comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens (IDs): tensor([[  785,  3974, 13876, 38835, 34208,   916,   279, 15678]],\n",
      "       device='mps:0')\n",
      "Decoded inputs: ['The', ' quick', ' brown', ' fox', ' jumps', ' over', ' the', ' lazy']\n",
      "\n",
      "Logits shape: torch.Size([1, 8, 151936])\n",
      "Shape meaning: (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The quick brown fox jumps over the lazy\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"Input tokens (IDs):\", inputs[\"input_ids\"])\n",
    "print(\"Decoded inputs:\", [tokenizer.decode(t) for t in inputs[\"input_ids\"][0]])\n",
    "\n",
    "# Run the forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(f\"\\nLogits shape: {logits.shape}\")\n",
    "print(\"Shape meaning: (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits shape tells us we have a score for *every* position in the input sequence. We only care about predicting the *next* word, so we want the logits for the **very last position** in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token logits shape: torch.Size([151936]) (One score for every token in vocabulary)\n",
      "\n",
      "Top 5 predictions for the next token:\n",
      "1. ' dog' (Token ID: 5562) - Probability: 0.8789\n",
      "2. '\n",
      "' (Token ID: 198) - Probability: 0.0171\n",
      "3. ' dogs' (Token ID: 12590) - Probability: 0.0118\n",
      "4. ' brown' (Token ID: 13876) - Probability: 0.0092\n",
      "5. ' (' (Token ID: 320) - Probability: 0.0086\n"
     ]
    }
   ],
   "source": [
    "# Get logits for the last token in the sequence\n",
    "next_token_logits = logits[0, -1, :]\n",
    "print(f\"Next token logits shape: {next_token_logits.shape} (One score for every token in vocabulary)\")\n",
    "\n",
    "# Convert raw logits into probabilities between 0 and 1 using Softmax\n",
    "probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Let's look at the top 5 most likely next tokens\n",
    "top_k = 5\n",
    "top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "print(\"\\nTop 5 predictions for the next token:\")\n",
    "for i in range(top_k):\n",
    "    token_id = top_indices[i].item()\n",
    "    prob = top_probs[i].item()\n",
    "    token_str = tokenizer.decode(token_id)\n",
    "    print(f\"{i+1}. '{token_str}' (Token ID: {token_id}) - Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implementing a Greedy Decoding Loop\n",
    "Greedy decoding means we *always* pick the token with the highest probability (the `argmax`). \n",
    "\n",
    "Let's build a loop to generate a whole sentence manually by constantly feeding our new tokens back into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Artificial intelligence is going to\n",
      "Generating:  be a big part of the future of the world. Itâ€™s already being used in many different ways, from self-driving cars to chatbots to virtual assistants. But what exactly is AI and how can\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def greedy_generate(prompt, max_new_tokens=20):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Generating: \", end=\"\")\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        # 1. Get logits of the very last token\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # 2. Choose the one with the highest probability (argmax)\n",
    "        next_token_id = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # 3. Decode and print it out as we go\n",
    "        next_word = tokenizer.decode(next_token_id[0])\n",
    "        print(next_word, end=\"\", flush=True)\n",
    "        \n",
    "        # 4. Append token to our input IDs and repeat the loop!\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "        \n",
    "    print(\"\\n\\nDone!\")\n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Try it out! Notice how plain greedy decoding can sometimes get stuck in repetitive loops.\n",
    "generated_text = greedy_generate(\"Artificial intelligence is going to\", max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Experimentation Ideas\n",
    "\n",
    "1. **Change the prompt:** from factual to creative (e.g., â€œThe theory of relativity statesâ€¦â€ vs â€œOnce upon a timeâ€¦â€).\n",
    "   * *Does greedy decoding behave differently?*\n",
    "2. **Print the top 10 tokens and their probabilities:** at each step.\n",
    "   * *Is the highest probability much larger than the others?*\n",
    "3. **Measure entropy of the next-token distribution:** (using the `metrics.py` module).\n",
    "   * *Does entropy decrease as the sentence progresses?*\n",
    "4. **Force generation to continue for 100+ tokens:**\n",
    "   * *Observe repetition or looping behavior.*\n",
    "5. **Compare your manual loop with `model.generate()` using greedy mode:**\n",
    "   * *Are they identical?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment 1 & 4: Creative Prompt vs Factual Prompt (100 tokens) ===\n",
      "\n",
      "--- Creative ---\n",
      "Prompt: Once upon a time in a magical\n",
      "Generating:  kingdom, there lived a wise wizard named Merlin. He was known for his ability to predict the weather with incredible accuracy. One day, he decided to test his prediction skills by predicting the weather for the next 10 days. He used a special formula to calculate the probability of rain for each day, based on the weather patterns of the previous 10 days. The formula was as follows:\n",
      "\n",
      "\\[ P(\\text{Rain on day } n) = \\frac{1}{n+1\n",
      "\n",
      "Done!\n",
      "\n",
      "--- Factual ---\n",
      "Prompt: The theory of relativity states that\n",
      "Generating:  the speed of light is constant in all inertial frames of reference. This means that if you are moving at a constant speed, the speed of light will appear to slow down for you. This is known as the Lorentz transformation. The Lorentz transformation is a set of equations that describe how the speed of light changes in different reference frames. It is based on the fact that the speed of light is a constant, and that it is the same in all inertial frames of reference.\n",
      "\n",
      "Done!\n",
      "\n",
      "=== Experiment 2 & 3: Top 10 Tokens, Probabilities, and Entropy ===\n",
      "\n",
      "Step 1 Entropy: 5.3750\n",
      "  1. ' the': 0.2451\n",
      "  2. ' time': 0.1021\n",
      "  3. ' all': 0.0547\n",
      "  4. ' space': 0.0483\n",
      "  5. ',': 0.0427\n",
      "  6. ' mass': 0.0293\n",
      "  7. ' a': 0.0228\n",
      "  8. ' as': 0.0228\n",
      "  9. ' for': 0.0201\n",
      "  10. ' in': 0.0178\n",
      "\n",
      "Step 2 Entropy: 4.5312\n",
      "  1. ' speed': 0.4316\n",
      "  2. ' laws': 0.1582\n",
      "  3. ' faster': 0.0215\n",
      "  4. ' mass': 0.0189\n",
      "  5. ' greater': 0.0167\n",
      "  6. ' fundamental': 0.0148\n",
      "  7. ' size': 0.0139\n",
      "  8. ' effects': 0.0101\n",
      "  9. ' velocity': 0.0095\n",
      "  10. ' basic': 0.0084\n",
      "\n",
      "Step 3 Entropy: 0.2432\n",
      "  1. ' of': 0.9766\n",
      "  2. ' at': 0.0123\n",
      "  3. ' limit': 0.0027\n",
      "  4. ' in': 0.0008\n",
      "  5. ' and': 0.0008\n",
      "  6. ',': 0.0006\n",
      "  7. ' limits': 0.0006\n",
      "  8. ' (': 0.0005\n",
      "  9. ' is': 0.0004\n",
      "  10. ' c': 0.0003\n",
      "\n",
      "Step 4 Entropy: 1.2266\n",
      "  1. ' light': 0.8594\n",
      "  2. ' a': 0.0294\n",
      "  3. ' any': 0.0294\n",
      "  4. ' an': 0.0178\n",
      "  5. ' all': 0.0139\n",
      "  6. ' information': 0.0051\n",
      "  7. ' the': 0.0035\n",
      "  8. ' matter': 0.0031\n",
      "  9. ' objects': 0.0031\n",
      "  10. ' sound': 0.0027\n",
      "\n",
      "Step 5 Entropy: 2.3438\n",
      "  1. ' is': 0.4648\n",
      "  2. ' in': 0.3613\n",
      "  3. ',': 0.0491\n",
      "  4. ' cannot': 0.0159\n",
      "  5. ' c': 0.0140\n",
      "  6. ' (': 0.0140\n",
      "  7. ' can': 0.0085\n",
      "  8. ' and': 0.0066\n",
      "  9. ' does': 0.0059\n",
      "  10. ' has': 0.0035\n",
      "\n",
      "=== Experiment 5: Compare with model.generate() ===\n",
      "\n",
      "HuggingFace default generate(): The theory of relativity states that the speed of light is constant in all inertial frames of reference. This means that if you are\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "from metrics import calculate_entropy\n",
    "\n",
    "print(\"=== Experiment 1 & 4: Creative Prompt vs Factual Prompt (100 tokens) ===\")\n",
    "creative_prompt = \"Once upon a time in a magical\"\n",
    "factual_prompt = \"The theory of relativity states that\"\n",
    "\n",
    "print(\"\\n--- Creative ---\")\n",
    "creative_output = greedy_generate(creative_prompt, max_new_tokens=100)\n",
    "print(\"\\n--- Factual ---\")\n",
    "factual_output = greedy_generate(factual_prompt, max_new_tokens=100)\n",
    "\n",
    "print(\"\\n=== Experiment 2 & 3: Top 10 Tokens, Probabilities, and Entropy ===\")\n",
    "# We will just run 5 steps to see how the distribution changes\n",
    "test_prompt = \"The theory of relativity states that\"\n",
    "input_ids = tokenizer(test_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "for step in range(5):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    entropy = calculate_entropy(probs)\n",
    "    print(f\"\\nStep {step+1} Entropy: {entropy:.4f}\")\n",
    "    \n",
    "    top_probs, top_indices = torch.topk(probs, 10)\n",
    "    for i in range(10):\n",
    "        token_str = repr(tokenizer.decode(top_indices[i].item()))\n",
    "        print(f\"  {i+1}. {token_str}: {top_probs[i].item():.4f}\")\n",
    "        \n",
    "    # Append the greedy choice to continue\n",
    "    next_token = torch.argmax(logits).unsqueeze(0).unsqueeze(0)\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "print(\"\\n=== Experiment 5: Compare with model.generate() ===\")\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Greedy by default (do_sample=False)\n",
    "outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "hf_generated = tokenizer.decode(outputs[0])\n",
    "print(f\"\\nHuggingFace default generate(): {hf_generated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The Greedy Trap ---\n",
      "Prompt: 'A cat is a cat is a'\n",
      "\n",
      "Tracking the next 5 tokens, their probabilities, and the distribution entropy:\n",
      "\n",
      "[Step 1] Entropy: 6.9375\n",
      "  1. ' cat': 0.2158 <-- Greedy Choice\n",
      "  2. ' dog': 0.1226 \n",
      "  3. ' mouse': 0.0659 \n",
      "\n",
      "[Step 2] Entropy: 4.0312\n",
      "  1. ' is': 0.4688 <-- Greedy Choice\n",
      "  2. '.': 0.1523 \n",
      "  3. ',': 0.0718 \n",
      "\n",
      "[Step 3] Entropy: 2.4062\n",
      "  1. ' a': 0.8047 <-- Greedy Choice\n",
      "  2. ' what': 0.0115 \n",
      "  3. ' the': 0.0095 \n",
      "\n",
      "[Step 4] Entropy: 2.8906\n",
      "  1. ' cat': 0.7578 <-- Greedy Choice\n",
      "  2. ' dog': 0.0203 \n",
      "  3. ' very': 0.0167 \n",
      "\n",
      "[Step 5] Entropy: 3.7344\n",
      "  1. ' is': 0.4727 <-- Greedy Choice\n",
      "  2. '.': 0.1533 \n",
      "  3. '\\n': 0.0498 \n",
      "\n",
      "--- Generating 30 more tokens to see the full loop ---\n",
      "A cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is\n",
      "... stuck forever!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "from metrics import calculate_entropy\n",
    "\n",
    "# A prompt designed to trap greedy decoding into a loop\n",
    "loop_prompt = \"A cat is a cat is a\"\n",
    "input_ids = tokenizer(loop_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "print(f\"--- The Greedy Trap ---\")\n",
    "print(f\"Prompt: {repr(loop_prompt)}\\n\")\n",
    "print(\"Tracking the next 5 tokens, their probabilities, and the distribution entropy:\")\n",
    "\n",
    "for step in range(5):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    \n",
    "    logits = outputs.logits[0, -1, :]\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    entropy = calculate_entropy(probs)\n",
    "    \n",
    "    # We want to see the top 3 choices to understand how the model is narrowing down\n",
    "    top_probs, top_indices = torch.topk(probs, 3)\n",
    "    \n",
    "    print(f\"\\n[Step {step+1}] Entropy: {entropy:.4f}\")\n",
    "    for i in range(3):\n",
    "        token_str = repr(tokenizer.decode(top_indices[i].item()))\n",
    "        print(f\"  {i+1}. {token_str}: {top_probs[i].item():.4f} \" + (\"<-- Greedy Choice\" if i == 0 else \"\"))\n",
    "        \n",
    "    # Append the greedy choice to continue the loop\n",
    "    next_token = top_indices[0].unsqueeze(0).unsqueeze(0)\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "print(\"\\n--- Generating 30 more tokens to see the full loop ---\")\n",
    "print(tokenizer.decode(input_ids[0]), end=\"\") # Print what we have so far\n",
    "for _ in range(30):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    next_token = torch.argmax(outputs.logits[0, -1, :]).unsqueeze(0).unsqueeze(0)\n",
    "    print(tokenizer.decode(next_token[0]), end=\"\", flush=True)\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "print(\"\\n... stuck forever!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
