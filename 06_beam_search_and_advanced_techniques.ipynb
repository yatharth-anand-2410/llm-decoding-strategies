{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Beam Search and Advanced Optimization\n",
    "\n",
    "Everything we have done so far (Greedy, Temperature, Top-K, Top-P) has been **Local Optimization**.\n",
    "\n",
    "At every single step, we look *only* at the very next token and try to make the best choice. This is fast, but it can lead us down a blind alley. What if choosing a slightly sub-optimal token *now* opens up a highly probable, brilliant grammatical structure three words from now?\n",
    "\n",
    "**Beam Search** is a form of **Global Sequence Optimization**.\n",
    "\n",
    "Instead of keeping just one sequence, we maintain $B$ active \"beams\" (hypotheses). At each step, we expand all $B$ beams with all possible next tokens, score the resulting sentences by multiplying all probabilities together, and keep only the top $B$ sentences to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770df7d90aa048f8b341978ef96b416e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import math\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Why Log-Probabilities?\n",
    "\n",
    "If you multiply probabilities together (e.g., `0.1 * 0.1 * 0.1`), the number quickly becomes infinitesimally small, leading to floating-point underflow on computers.\n",
    "\n",
    "Instead of multiplying probabilities, we **add** their logarithms.\n",
    "\n",
    "$\\log(A \\times B) = \\log(A) + \\log(B)$\n",
    "\n",
    "Because probabilities are between 0 and 1, their logs are always negative. The closer to 0 (the less negative), the more probable the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Combined Prob: 0.05\n",
      "Log Combined Prob: -2.995732273553991 (Notice it is negative!)\n"
     ]
    }
   ],
   "source": [
    "probability_1 = 0.5\n",
    "probability_2 = 0.1\n",
    "\n",
    "# Multiplication\n",
    "combined_prob = probability_1 * probability_2\n",
    "print(f\"Raw Combined Prob: {combined_prob}\")\n",
    "\n",
    "# Log Addition\n",
    "log_prob_1 = math.log(probability_1)\n",
    "log_prob_2 = math.log(probability_2)\n",
    "\n",
    "combined_log_prob = log_prob_1 + log_prob_2\n",
    "print(f\"Log Combined Prob: {combined_log_prob} (Notice it is negative!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implementing a simple Beam Search\n",
    "\n",
    "Let's implement a simplified Beam Search. \n",
    "\n",
    "Note that true production Beam Search relies heavily on batching tensors to be fast. We will write a more readable, looped version to understand the exact logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The chef prepared a' (Beam Width: 1)\n",
      "\n",
      "--- Step 1 top candidates ---\n",
      "Beam 1 [Score: -0.4707]: 'The chef prepared a large'\n",
      "\n",
      "--- Step 2 top candidates ---\n",
      "Beam 1 [Score: -0.8828]: 'The chef prepared a large batch'\n",
      "\n",
      "--- Step 3 top candidates ---\n",
      "Beam 1 [Score: -0.8892]: 'The chef prepared a large batch of'\n",
      "\n",
      "--- Step 4 top candidates ---\n",
      "Beam 1 [Score: -3.2017]: 'The chef prepared a large batch of cookies'\n",
      "\n",
      "--- Step 5 top candidates ---\n",
      "Beam 1 [Score: -4.0376]: 'The chef prepared a large batch of cookies for'\n",
      "\n",
      "\n",
      "=== Final Winner ===\n",
      "\n",
      "===========================\n",
      "\n",
      "Prompt: 'The chef prepared a' (Beam Width: 3)\n",
      "\n",
      "--- Step 1 top candidates ---\n",
      "Beam 1 [Score: -0.4707]: 'The chef prepared a large'\n",
      "Beam 2 [Score: -2.2188]: 'The chef prepared a batch'\n",
      "Beam 3 [Score: -3.3438]: 'The chef prepared a total'\n",
      "\n",
      "--- Step 2 top candidates ---\n",
      "Beam 1 [Score: -0.8828]: 'The chef prepared a large batch'\n",
      "Beam 2 [Score: -2.2251]: 'The chef prepared a batch of'\n",
      "Beam 3 [Score: -2.6270]: 'The chef prepared a large number'\n",
      "\n",
      "--- Step 3 top candidates ---\n",
      "Beam 1 [Score: -0.8892]: 'The chef prepared a large batch of'\n",
      "Beam 2 [Score: -2.6326]: 'The chef prepared a large number of'\n",
      "Beam 3 [Score: -4.0298]: 'The chef prepared a batch of las'\n",
      "\n",
      "--- Step 4 top candidates ---\n",
      "Beam 1 [Score: -3.2017]: 'The chef prepared a large batch of cookies'\n",
      "Beam 2 [Score: -3.2642]: 'The chef prepared a large batch of las'\n",
      "Beam 3 [Score: -3.8892]: 'The chef prepared a large batch of soup'\n",
      "\n",
      "--- Step 5 top candidates ---\n",
      "Beam 1 [Score: -3.3384]: 'The chef prepared a large batch of lasagna'\n",
      "Beam 2 [Score: -4.0376]: 'The chef prepared a large batch of cookies for'\n",
      "Beam 3 [Score: -4.6626]: 'The chef prepared a large batch of cookies.'\n",
      "\n",
      "\n",
      "=== Final Winner ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The chef prepared a large batch of lasagna'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simplified_beam_search(prompt, beam_width=3, max_length=10, length_penalty=1.0):\n",
    "    print(f\"Prompt: '{prompt}' (Beam Width: {beam_width})\")\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # A \"beam\" is a tuple: (sequence_tensor, cumulative_log_prob)\n",
    "    # We start with 1 beam: our initial prompt, with a starting score of 0.0\n",
    "    beams = [(input_ids, 0.0)]\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        all_candidates = []\n",
    "        \n",
    "        # 1. Expand each active beam\n",
    "        for seq, score in beams:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(seq)\n",
    "                \n",
    "            # Get the logits for the last token\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Convert to log probabilities (`log_softmax` is numerically stable)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # To save massive compute, we don't actually expand the entire vocabulary.\n",
    "            # We only expand the top B most likely next tokens from this specific beam.\n",
    "            top_k_log_probs, top_k_indices = torch.topk(log_probs, beam_width)\n",
    "            \n",
    "            # 2. Gather candidates from this expansion\n",
    "            for i in range(beam_width):\n",
    "                next_token_id = top_k_indices[i].unsqueeze(0).unsqueeze(0)\n",
    "                token_log_prob = top_k_log_probs[i].item()\n",
    "                \n",
    "                # Append token to sequence\n",
    "                new_seq = torch.cat([seq, next_token_id], dim=-1)\n",
    "                \n",
    "                # Add the log prob (which is negative) to our running score\n",
    "                new_score = score + token_log_prob\n",
    "                \n",
    "                all_candidates.append((new_seq, new_score))\n",
    "                \n",
    "        # 3. Sort all candidates by score (descending)\n",
    "        # Apply length penalty to prevent the model from artificially favoring shorter sentences\n",
    "        # Score / (Length ^ penalty)\n",
    "        all_candidates = sorted(all_candidates, key=lambda x: x[1] / (x[0].shape[1] ** length_penalty), reverse=True)\n",
    "        \n",
    "        # 4. Prune back down to `beam_width`\n",
    "        beams = all_candidates[:beam_width]\n",
    "        \n",
    "        # Print debug info to watch the beams fight for survival\n",
    "        print(f\"\\n--- Step {step+1} top candidates ---\")\n",
    "        for i, (seq, score) in enumerate(beams):\n",
    "            text = tokenizer.decode(seq[0])\n",
    "            print(f\"Beam {i+1} [Score: {score:.4f}]: {repr(text)}\")\n",
    "            \n",
    "    print(\"\\n\\n=== Final Winner ===\")\n",
    "    final_seq, final_score = beams[0]\n",
    "    return tokenizer.decode(final_seq[0])\n",
    "\n",
    "# Try it with a beam width of 1 (which equals Greedy decoding)\n",
    "simplified_beam_search(\"The chef prepared a\", beam_width=1, max_length=5)\n",
    "\n",
    "print(\"\\n===========================\\n\")\n",
    "\n",
    "# Try it with a beam width of 3\n",
    "simplified_beam_search(\"The chef prepared a\", beam_width=3, max_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Experimentation Ideas\n",
    "\n",
    "1. **Compare Beam Width = 1 (greedy), 3, 5, 10:**\n",
    "   * *Notice how execution time slows down dramatically as the tree branches out.*\n",
    "2. **Track cumulative log-probability of each beam:**\n",
    "   * *Look at the debug prints at Step 5. Did the sequence that was winning at Step 1 actually end up being the final winner? (This proves why Beam Search works where Greedy fails).* \n",
    "3. **Compare Beam Search vs Top-P for:**\n",
    "   * *Translation-style prompts: \"Translate English to French: Hello World ->\"*\n",
    "   * *Creative storytelling: \"Write a poem about a robot:\"*\n",
    "   * *(Hint: Beam Search is heavily preferred for deterministic tasks like translation, but Top-P is preferred for creative chatting. Why?)*\n",
    "4. **Experiment with length penalty values (0.6, 1.0, 1.5):**\n",
    "   * *Modify the lambda func in step 3. High penalty forces longer sentences. Low penalty favors short ones. Observe when beam search produces shorter outputs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af372a9fc5f14c9d9da1924b9713a73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment 1: Execution Time vs Beam Width ===\n",
      "\n",
      "Prompt: 'The key to making a great pizza is' (Beam Width: 1, Length Pen: 1.0)\n",
      "Final Output: The key to making a great pizza is the right dough. The dough is the base of the pizza, and it\n",
      "-> Time Taken: 7.12 seconds\n",
      "\n",
      "Prompt: 'The key to making a great pizza is' (Beam Width: 3, Length Pen: 1.0)\n",
      "Final Output: The key to making a great pizza is to have the right ingredients, the right proportions, and the right technique.\n",
      "-> Time Taken: 3.28 seconds\n",
      "\n",
      "Prompt: 'The key to making a great pizza is' (Beam Width: 5, Length Pen: 1.0)\n",
      "Final Output: The key to making a great pizza is using the right ingredients and techniques. Here are some tips to help you create\n",
      "-> Time Taken: 6.27 seconds\n",
      "\n",
      "=== Experiment 2 & 3: Translation vs Creative ===\n",
      "Top-P excels at Creative tasks due to randomness. Beam excels at Translation because there is usually one 'optimum' path.\n",
      "\n",
      "--- Translation with Beam (Usually accurate) ---\n",
      "\n",
      "Prompt: 'Translate English to French: Hello World ->' (Beam Width: 5, Length Pen: 1.0)\n",
      "Final Output: Translate English to French: Hello World -> Bonjour le monde\n",
      "Bonjour le monde!<|endoftext|>\n",
      "\n",
      "--- Creative with Beam (Usually boring/repetitive) ---\n",
      "\n",
      "Prompt: 'Write a short poem about a robot:' (Beam Width: 5, Length Pen: 1.0)\n",
      "Final Output: Write a short poem about a robot: Title: The Unseen Companion\n",
      "\n",
      "In the vast expanse of space,\n",
      "\n",
      "\n",
      "=== Experiment 4: Length Penalties ===\n",
      "High penalty (>1.0) artificially boosts scores of long sentences. Low penalty (<1.0) punishes them.\n",
      "\n",
      "Length Penalty 0.6 (Favors Short):\n",
      "\n",
      "Prompt: 'The key to making a great pizza is' (Beam Width: 3, Length Pen: 0.6)\n",
      "Final Output: The key to making a great pizza is to have the right ingredients, the right proportions, and the right way to cook it. Here are\n",
      "\n",
      "Length Penalty 1.5 (Favors Long):\n",
      "\n",
      "Prompt: 'The key to making a great pizza is' (Beam Width: 3, Length Pen: 1.5)\n",
      "Final Output: The key to making a great pizza is to have the right ingredients, the right proportions, and the right way to cook it. Here are\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The key to making a great pizza is to have the right ingredients, the right proportions, and the right way to cook it. Here are'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "# Local, independent function for the cell\n",
    "def simplified_beam_search_local(prompt, beam_width=3, max_length=10, length_penalty=1.0):\n",
    "    print(f\"\\nPrompt: '{prompt}' (Beam Width: {beam_width}, Length Pen: {length_penalty})\")\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    beams = [(input_ids, 0.0)]\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        all_candidates = []\n",
    "        for seq, score in beams:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(seq)\n",
    "            log_probs = F.log_softmax(outputs.logits[0, -1, :], dim=-1)\n",
    "            top_k_log_probs, top_k_indices = torch.topk(log_probs, beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                new_seq = torch.cat([seq, top_k_indices[i].unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "                new_score = score + top_k_log_probs[i].item()\n",
    "                all_candidates.append((new_seq, new_score))\n",
    "                \n",
    "        # The length penalty experiment is here:\n",
    "        all_candidates = sorted(all_candidates, key=lambda x: x[1] / (x[0].shape[1] ** length_penalty), reverse=True)\n",
    "        beams = all_candidates[:beam_width]\n",
    "        \n",
    "    final_seq, final_score = beams[0]\n",
    "    print(f\"Final Output: {tokenizer.decode(final_seq[0])}\")\n",
    "    return tokenizer.decode(final_seq[0])\n",
    "\n",
    "print(\"=== Experiment 1: Execution Time vs Beam Width ===\")\n",
    "test_prompt = \"The key to making a great pizza is\"\n",
    "for bw in [1, 3, 5]:\n",
    "    start_time = time.time()\n",
    "    simplified_beam_search_local(test_prompt, beam_width=bw, max_length=15)\n",
    "    print(f\"-> Time Taken: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n=== Experiment 2 & 3: Translation vs Creative ===\")\n",
    "print(\"Top-P excels at Creative tasks due to randomness. Beam excels at Translation because there is usually one 'optimum' path.\")\n",
    "translation_prompt = \"Translate English to French: Hello World ->\"\n",
    "creative_prompt = \"Write a short poem about a robot:\"\n",
    "\n",
    "print(\"\\n--- Translation with Beam (Usually accurate) ---\")\n",
    "simplified_beam_search_local(translation_prompt, beam_width=5, max_length=10)\n",
    "\n",
    "print(\"\\n--- Creative with Beam (Usually boring/repetitive) ---\")\n",
    "simplified_beam_search_local(creative_prompt, beam_width=5, max_length=15)\n",
    "\n",
    "print(\"\\n=== Experiment 4: Length Penalties ===\")\n",
    "print(\"High penalty (>1.0) artificially boosts scores of long sentences. Low penalty (<1.0) punishes them.\")\n",
    "print(\"\\nLength Penalty 0.6 (Favors Short):\")\n",
    "simplified_beam_search_local(test_prompt, beam_width=3, max_length=20, length_penalty=0.6)\n",
    "\n",
    "print(\"\\nLength Penalty 1.5 (Favors Long):\")\n",
    "simplified_beam_search_local(test_prompt, beam_width=3, max_length=20, length_penalty=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
