{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Contrastive Search\n",
    "\n",
    "Welcome to one of the most brilliant State-Of-The-Art (SOTA) deterministic decoding methods: **Contrastive Search**.\n",
    "\n",
    "In notebooks 1-6, we explored the tension between **Determinism (Greedy, Beam)** and **Stochasticity (Temperature, Top-P)**.\n",
    "- **Greedy** was too boring and trapped itself in infinite loops.\n",
    "- **Top-P/Temperature** was wildly creative, but ran the risk of hallucination and incoherence.\n",
    "\n",
    "In 2022, researchers asked: *Can we get the high coherence of Greedy Decoding, without ever falling into a repetitive loop, but also without relying on random dice rolls?*\n",
    "\n",
    "The answer was Contrastive Search.\n",
    "\n",
    "## How it works (The Math)\n",
    "\n",
    "Contrastive Search evaluates the top `K` most likely next words. For each word, it calculates a final score based on two competing factors:\n",
    "\n",
    "1. **Model Confidence:** How mathematically likely is this token? (We want this to be HIGH).\n",
    "2. **Degeneration Penalty:** How mathematically similar is the definition/embedding of this token to the definitions of the tokens we have *already* written? (We want this to be LOW).\n",
    "\n",
    "$$ \\text{Score}(v) = (1 - \\alpha) \\times \\text{Confidence}(v) - \\alpha \\times \\text{Similarity}(v, \\text{past}) $$\n",
    "\n",
    "The hyperparameter $\\alpha$ (Alpha) balances the two. If $\\alpha = 0.0$, this is identical to Greedy Decoding. If $\\alpha = 1.0$, the model cares *only* about avoiding repetition.\n",
    "\n",
    "Because it uses **Cosine Similarity** on the deep neural states of the tokens, it understands synonyms! If you say \"The large dog\", it knows that \"big\" is mathematically similar to \"large\", and will penalize it, forcing the model to pick a more interesting word to keep the story moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-0.5B on mps...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be102f57be64edebdf294c99b9e021a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "print(f\"Loading {model_id} on {device}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model.eval()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Implementation\n",
    "\n",
    "HuggingFace has actually built Contrastive Search natively into `model.generate()`. To use it, you must provide exactly two parameters:\n",
    "1. `penalty_alpha` (> 0.0)\n",
    "2. `top_k` (> 1)\n",
    "\n",
    "Let's see it in action against a prompt that usually destroys Greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4e271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Greedy Decoding (alpha=0.0) ---\n",
      "A cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat is a cat\n",
      "\n",
      "--- Contrastive Search (alpha=0.6, top_k=4) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55da6cd6b4745bf85fdc6b3bf1f9dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generate.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/transformers-community/contrastive-search:\n",
      "- custom_generate/generate.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Passing `generation_config` together with generation-related arguments=({'penalty_alpha', 'top_k', 'max_new_tokens', 'cache_implementation', 'pad_token_id'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "An assistant model is provided, using a dynamic cache instead of a cache of type='dynamic_full'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is a cat is a mammal.\n",
      "This justifies what answer for what question? Q & A:\n",
      "Question: Is a cat a mammal?\n",
      "\n",
      "Answer: Yes, a cat is a mammal.\n",
      "You are an AI\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A cat is a cat is a\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"--- Greedy Decoding (alpha=0.0) ---\")\n",
    "greedy_out = model.generate(**inputs, max_new_tokens=40, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(greedy_out[0]))\n",
    "\n",
    "print(\"\\n--- Contrastive Search (alpha=0.6, top_k=4) ---\")\n",
    "# Notice we are NOT using do_sample=True! Contrastive search is deterministic.\n",
    "contrast_out = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=40, \n",
    "    penalty_alpha=0.6, \n",
    "    top_k=4,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(tokenizer.decode(contrast_out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”¬ Experimentation Ideas\n",
    "\n",
    "1. **Sweep Alpha Values:** \n",
    "   * *Run generation with `penalty_alpha` set to 0.1, 0.5, and 0.9. Watch how the text changes from repetitive to almost overly-verbose as it desperately avoids similar ideas.*\n",
    "2. **Combine with Top-K:**\n",
    "   * *What happens if `top_k = 2` vs `top_k = 50`? Contrastive search only evaluates the K most likely tokens. If K is too high, does it pick a terrible, grammatically incorrect word just because it is highly dissimilar to the context?*\n",
    "3. **Compare to Repetition Penalty (Notebook 5):**\n",
    "   * *Contrastive search penalizes deep semantic similarity (hidden states). Repetition Penalty penalizes exact string matches (token IDs). Which one produces better creative writing?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5c503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment 1: Sweep Alpha Values ===\n",
      "\n",
      "--- Alpha = 0.1 (Top-K = 4) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spaceship landed on the alien planet and the crew decided to count the number of alien species. They found that there were 120 alien species in total. If they counted 30 alien species in the first hour and 20 more\n",
      "\n",
      "--- Alpha = 0.5 (Top-K = 4) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spaceship landed on the alien planet and the crew decided to count the number of stars in the night sky. They found that there were 120 stars visible from the spaceship. If each star is represented by a point on a coordinate grid,\n",
      "\n",
      "--- Alpha = 0.9 (Top-K = 4) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spaceship landed on the alien planet and the crew was divided into two teams to explore a new area. Team 1 consisted 1/3 of the crew, which is 40% of the total crew. How many people are part of\n",
      "\n",
      "\n",
      "=== Experiment 2: Combine with Top-K ===\n",
      "Observe what happens when Top-K is huge. The model might pick a grammatically terrible word just because it is highly mathematically dissimilar to the context.\n",
      "\n",
      "--- Top-K = 2 (Alpha = 0.6) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spaceship landed on the alien planet and the crew decided to count the number of aliens they saw. They saw 12 aliens on the first day and 15 aliens on the second day. How many aliens did they see in total over the\n",
      "\n",
      "--- Top-K = 5 (Alpha = 0.6) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spaceship landed on the alien planet and the crew was divided into two teams. Team $A$ consisted of 10 scientists and 5 mathematicians, while Team $B$ consisted of 8 scientists and 6 mathematicians. After a\n",
      "\n",
      "--- Top-K = 50 (Alpha = 0.6) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spaceship landed on the alien planet and the crew went ashore.\n",
      "Generate a new sentence that is, on a scale from 0 to 5, a 4 in textual similarity to the above sentence altough it is nonsensical or fl\n",
      "\n",
      "\n",
      "=== Experiment 3: Contrastive Search vs Repetition Penalty ===\n",
      "\n",
      "--- Contrastive Search (Semantic similarity penalty) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old wizard walked up to the magical glowing orb and said \"I am the source of all magic. I can turn any object into anything else, and I can create a portal to any other world.\" The wizard then disappeared into the orb, and a portal opened up in the sky.  Given the\n",
      "\n",
      "--- Repetition Penalty (Exact token match penalty) ---\n",
      "The old wizard walked up to the magical glowing orb and cast it on a tiny, silver sphere.\n",
      "In his absence he heard two young elves whispering about their new home in this realm of magic.\n",
      "\n",
      "[img src=\"https://i.stack.imgur.com/kZ8rR.jpg\" alt=\"\" border=\"_\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Experiment 1: Sweep Alpha Values ===\")\n",
    "# We use a narrative prompt to see how creatively it avoids repeating itself\n",
    "prompt = \"The spaceship landed on the alien planet and the crew\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Alpha = 0.1 (Very little penalty) vs 0.9 (Massive penalty for similarity)\n",
    "for alpha in [0.1, 0.5, 0.9]:\n",
    "    print(f\"\\n--- Alpha = {alpha} (Top-K = 4) ---\")\n",
    "    out = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=40, \n",
    "        penalty_alpha=alpha, \n",
    "        top_k=4, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(tokenizer.decode(out[0]))\n",
    "\n",
    "print(\"\\n\\n=== Experiment 2: Combine with Top-K ===\")\n",
    "print(\"Observe what happens when Top-K is huge. The model might pick a grammatically terrible word just because it is highly mathematically dissimilar to the context.\")\n",
    "for k_val in [2, 5, 50]:\n",
    "    print(f\"\\n--- Top-K = {k_val} (Alpha = 0.6) ---\")\n",
    "    out = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=40, \n",
    "        penalty_alpha=0.6, \n",
    "        top_k=k_val, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(tokenizer.decode(out[0]))\n",
    "\n",
    "print(\"\\n\\n=== Experiment 3: Contrastive Search vs Repetition Penalty ===\")\n",
    "creative_prompt = \"The old wizard walked up to the magical glowing orb and\"\n",
    "creative_inputs = tokenizer(creative_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(\"\\n--- Contrastive Search (Semantic similarity penalty) ---\")\n",
    "contrast_out = model.generate(\n",
    "    **creative_inputs, \n",
    "    max_new_tokens=50, \n",
    "    penalty_alpha=0.6, \n",
    "    top_k=4, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(tokenizer.decode(contrast_out[0]))\n",
    "\n",
    "print(\"\\n--- Repetition Penalty (Exact token match penalty) ---\")\n",
    "# To make it a fair fight for creative writing, we give the repetition penalty some random sampling\n",
    "rep_out = model.generate(\n",
    "    **creative_inputs, \n",
    "    max_new_tokens=50, \n",
    "    repetition_penalty=1.5,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(rep_out[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05bcca",
   "metadata": {},
   "source": [
    "# Mannual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ea4788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Manual Implementation of Contrastive Search ===\n",
      "Prompt: A cat is a cat is a\n",
      "Generating:  mammal.\n",
      "This justifies what answer for what question? Q & A: Question: What is a mammal?\n",
      "\n",
      "Answer: Mammals are a group of animals that have a common characteristic,\n",
      "\n",
      "Done! Notice how our manual implementation perfectly avoids the 'cat is a' loop.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=== Manual Implementation of Contrastive Search ===\")\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.6\n",
    "k = 4\n",
    "max_tokens = 40\n",
    "\n",
    "prompt = \"A cat is a cat is a\"\n",
    "\n",
    "# FIX: We must explicitly extract .input_ids from the tokenizer output\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Generating: \", end=\"\")\n",
    "\n",
    "for step in range(max_tokens):\n",
    "    with torch.no_grad():\n",
    "        # output_hidden_states=True is the secret to contrastive search!\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "    \n",
    "    # 1. Get the raw logits and convert to probabilities\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # 2. Find the Top-K most likely candidates (Model Confidence)\n",
    "    top_k_probs, top_k_indices = torch.topk(next_token_probs, k)\n",
    "    \n",
    "    # We need the past hidden states to compare against\n",
    "    # outputs.hidden_states is a tuple of all layers. We want the very last layer [-1]\n",
    "    # Shape: [batch, sequence_length, hidden_dimension]\n",
    "    past_hidden_states = outputs.hidden_states[-1][0] \n",
    "    \n",
    "    best_score = -float('Inf')\n",
    "    best_token = None\n",
    "    \n",
    "    # 3. Evaluate each of the K candidates\n",
    "    for i in range(k):\n",
    "        candidate_token = top_k_indices[i].unsqueeze(0).unsqueeze(0)\n",
    "        candidate_prob = top_k_probs[i].item()\n",
    "        \n",
    "        # To get the hidden state of the candidate, we append it \n",
    "        # and do a quick forward pass just for this token\n",
    "        test_input = torch.cat([input_ids, candidate_token], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(test_input, output_hidden_states=True)\n",
    "        \n",
    "        # The hidden meaning of our candidate token \n",
    "        candidate_hidden_state = test_outputs.hidden_states[-1][0, -1, :]\n",
    "        \n",
    "        # 4. Calculate Cosine Similarity against ALL past tokens\n",
    "        # We want to find the single past token that is MOST similar to our candidate\n",
    "        similarities = []\n",
    "        for past_token_state in past_hidden_states:\n",
    "            sim = F.cosine_similarity(candidate_hidden_state, past_token_state, dim=0)\n",
    "            similarities.append(sim.item())\n",
    "            \n",
    "        max_similarity = max(similarities)\n",
    "        \n",
    "        # 5. THE CONTRASTIVE SEARCH EQUATION\n",
    "        score = (1.0 - alpha) * candidate_prob - (alpha * max_similarity)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_token = candidate_token\n",
    "            \n",
    "    # Append the winning token and continue the loop!\n",
    "    input_ids = torch.cat([input_ids, best_token], dim=-1)\n",
    "    print(tokenizer.decode(best_token[0]), end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDone! Notice how our manual implementation perfectly avoids the 'cat is a' loop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737a4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
