{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 8. Constrained / Structured Decoding\n",
                "\n",
                "Have you ever wondered how APIs like OpenAI's `response_format={ \"type\": \"json_object\" }` actually guarantee that the output is flawless JSON 100% of the time?\n",
                "\n",
                "They use **Constrained Decoding** (often powered by libraries like `guidance`, `outlines`, or `llama.cpp` grammars).\n",
                "\n",
                "This is arguably the most important concept in modern agentic AI. If you want an LLM to call a Python function, you cannot tolerate a hallucinated quotation mark or a missing bracket. The syntax must be mathematically guaranteed.\n",
                "\n",
                "## The Concept\n",
                "\n",
                "Instead of begging the model in the prompt (\"PLEASE output valid JSON, do not include markdown, I will tip you $200\"), Constrained Decoding intercepts the raw **Logits** right before they are converted into probabilities via Softmax.\n",
                "\n",
                "It uses a deterministic parser (like a Regex engine or JSON schema validator) to analyze the string generated so far. It then asks the parser: *\"Which tokens in the vocabulary are legally allowed to come next?\"*\n",
                "\n",
                "Every illegal token has its logit score manually hard-coded to `-Infinity`. When Softmax is applied, `-Infinity` becomes $0.0\\%$. It becomes mathematically impossible for the model to hallucinate bad syntax."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6a001b7ff4d94595bcbf65e92bedca40",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ready for structured generation!\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
                "model.eval()\n",
                "\n",
                "print(\"Ready for structured generation!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A Simple Example: Forcing a Binary Choice\n",
                "\n",
                "Let's say we are building a sentiment analysis classifier. We ONLY want the model to output the literal word `\"Positive\"` or `\"Negative\"`. Nothing else is allowed.\n",
                "\n",
                "Instead of hoping the model obeys the prompt, we will brutally enforce it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Allowed Tokens -> Positive: 43903, Negative: 50857\n",
                        "\n",
                        "--- Top 3 Natural Model Choices (No Constraints) ---\n",
                        "' negative' (Logit: 15.69)\n",
                        "' Mixed' (Logit: 15.00)\n",
                        "' Negative' (Logit: 15.00)\n",
                        "\n",
                        "--- Applying the Constraint Mask ---\n",
                        "' Negative' (Logit: 15.0)\n",
                        "' Positive' (Logit: 11.875)\n",
                        "'!' (Logit: -inf)\n",
                        "\n",
                        "Final Output: Review: The food was absolutely disgusting and cold.\n",
                        "Sentiment: Negative\n"
                    ]
                }
            ],
            "source": [
                "# Find the exact Token IDs for the words we want to allow\n",
                "# Note: Tokenization is tricky! We prepend a space because words in the middle of a sentence often start with a space token.\n",
                "pos_id = tokenizer.encode(\" Positive\", add_special_tokens=False)[0]\n",
                "neg_id = tokenizer.encode(\" Negative\", add_special_tokens=False)[0]\n",
                "\n",
                "print(f\"Allowed Tokens -> Positive: {pos_id}, Negative: {neg_id}\")\n",
                "\n",
                "prompt = \"Review: The food was absolutely disgusting and cold.\\nSentiment:\"\n",
                "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model(input_ids)\n",
                "    \n",
                "logits = outputs.logits[0, -1, :]\n",
                "\n",
                "print(\"\\n--- Top 3 Natural Model Choices (No Constraints) ---\")\n",
                "top_values, top_indices = torch.topk(logits, 3)\n",
                "for i in range(3):\n",
                "    print(f\"{repr(tokenizer.decode(top_indices[i].item()))} (Logit: {top_values[i].item():.2f})\")\n",
                "\n",
                "print(\"\\n--- Applying the Constraint Mask ---\")\n",
                "# Create a mask of -Infinity\n",
                "mask = torch.full_like(logits, -float('Inf'))\n",
                "\n",
                "# Unmask ONLY the two allowed tokens!\n",
                "mask[pos_id] = logits[pos_id]\n",
                "mask[neg_id] = logits[neg_id]\n",
                "\n",
                "top_values_masked, top_indices_masked = torch.topk(mask, 3)\n",
                "for i in range(3):\n",
                "    val = top_values_masked[i].item()\n",
                "    print(f\"{repr(tokenizer.decode(top_indices_masked[i].item()))} (Logit: {val})\")\n",
                "\n",
                "# Generate the final forced token\n",
                "forced_token = torch.argmax(mask).unsqueeze(0).unsqueeze(0)\n",
                "print(f\"\\nFinal Output: {prompt}{tokenizer.decode(forced_token[0])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ”¬ Experimentation Ideas\n",
                "\n",
                "1. **Force a number generation:** \n",
                "   * *Find the token IDs for the digits `0` through `9`. Mask the logits so the model is mathematically forced to write a 10-digit phone number, even if the prompt asks it to write a poem.*\n",
                "2. **Implement a strict JSON array:**\n",
                "   * *Write a loop that forces the first token to be `[`, forces the middle tokens to be numbers or commas, and forces the final token to be `]`.*\n",
                "3. **Explore `transformers` LogitsProcessors:**\n",
                "   * *Look up HuggingFace's `LogitsProcessor` class. This is the exact mechanism they use under the hood to let you write custom python code that executes exactly where we built our `mask` above!*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "2f23b23d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Experiment 1: Forcing a 10-Digit Phone Number ===\n",
                        "Prompt: 'Write a beautiful, emotional poem about the moon and stars:\\n'\n",
                        "Generating: 1234567890\n",
                        "\n",
                        "=== Experiment 2: Strict JSON Array Generation ===\n",
                        "Prompt: 'The user data formatted as a JSON array:\\n'\n",
                        "Generating: [1,2,3]\n",
                        "\n",
                        "=== Experiment 3: Using HuggingFace LogitsProcessors ===\n",
                        "Instead of a manual loop, HuggingFace lets us inject our custom rules directly into model.generate()!\n",
                        "Prompt: The secret meaning of life is\n",
                        "Generating (Banning the letter 'e'):\n",
                        "The secret meaning of life is to find a way to ________ your own worth.\n",
                        "A. show\n",
                        "B. show off\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import LogitsProcessor, LogitsProcessorList\n",
                "\n",
                "print(\"=== Experiment 1: Forcing a 10-Digit Phone Number ===\")\n",
                "# 1. We must find the exact Token IDs for the digits 0-9 in the model's vocabulary\n",
                "digit_tokens = [str(i) for i in range(10)]\n",
                "digit_ids = [tokenizer.encode(d, add_special_tokens=False)[0] for d in digit_tokens]\n",
                "\n",
                "poem_prompt = \"Write a beautiful, emotional poem about the moon and stars:\\n\"\n",
                "input_ids = tokenizer(poem_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
                "\n",
                "print(f\"Prompt: {repr(poem_prompt)}\")\n",
                "print(\"Generating: \", end=\"\")\n",
                "\n",
                "# 2. We force exactly 10 generation steps\n",
                "for step in range(10):\n",
                "    with torch.no_grad():\n",
                "        outputs = model(input_ids)\n",
                "    \n",
                "    logits = outputs.logits[0, -1, :]\n",
                "    \n",
                "    # Create an absolute -Infinity mask\n",
                "    mask = torch.full_like(logits, -float('Inf'))\n",
                "    \n",
                "    # UNMASK ONLY the digits\n",
                "    for d_id in digit_ids:\n",
                "        mask[d_id] = logits[d_id]\n",
                "        \n",
                "    next_token = torch.argmax(mask).unsqueeze(0).unsqueeze(0)\n",
                "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
                "    print(tokenizer.decode(next_token[0]), end=\"\", flush=True)\n",
                "\n",
                "\n",
                "print(\"\\n\\n=== Experiment 2: Strict JSON Array Generation ===\")\n",
                "# 1. Token IDs for structural JSON characters\n",
                "bracket_open = tokenizer.encode(\"[\", add_special_tokens=False)[0]\n",
                "bracket_close = tokenizer.encode(\"]\", add_special_tokens=False)[0]\n",
                "comma = tokenizer.encode(\",\", add_special_tokens=False)[0]\n",
                "\n",
                "json_prompt = \"The user data formatted as a JSON array:\\n\"\n",
                "input_ids = tokenizer(json_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
                "\n",
                "# 2. We build a simple \"State Machine\" of arrays of allowed tokens at each specific step\n",
                "# Forcing the structure: [ digit , digit , digit ]\n",
                "allowed_sequences = [\n",
                "    [bracket_open],            # Step 0: Must be [\n",
                "    digit_ids,                 # Step 1: Must be any digit\n",
                "    [comma],                   # Step 2: Must be ,\n",
                "    digit_ids,                 # Step 3: Must be any digit\n",
                "    [comma],                   # Step 4: Must be ,\n",
                "    digit_ids,                 # Step 5: Must be any digit\n",
                "    [bracket_close]            # Step 6: Must be ]\n",
                "]\n",
                "\n",
                "print(f\"Prompt: {repr(json_prompt)}\")\n",
                "print(\"Generating: \", end=\"\")\n",
                "\n",
                "for step in range(len(allowed_sequences)):\n",
                "    with torch.no_grad():\n",
                "        outputs = model(input_ids)\n",
                "    logits = outputs.logits[0, -1, :]\n",
                "    \n",
                "    # Apply the mask for this specific step in our state machine\n",
                "    mask = torch.full_like(logits, -float('Inf'))\n",
                "    allowed_ids = allowed_sequences[step]\n",
                "    \n",
                "    for a_id in allowed_ids:\n",
                "        mask[a_id] = logits[a_id]\n",
                "        \n",
                "    next_token = torch.argmax(mask).unsqueeze(0).unsqueeze(0)\n",
                "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
                "    print(tokenizer.decode(next_token[0]), end=\"\", flush=True)\n",
                "\n",
                "\n",
                "print(\"\\n\\n=== Experiment 3: Using HuggingFace LogitsProcessors ===\")\n",
                "print(\"Instead of a manual loop, HuggingFace lets us inject our custom rules directly into model.generate()!\")\n",
                "\n",
                "# We create a custom class that inherits from LogitsProcessor\n",
                "class NoVowelsProcessor(LogitsProcessor):\n",
                "    \"\"\"A processor that bans the letter 'e'/'E' from ever being generated.\"\"\"\n",
                "    def __init__(self, tokenizer):\n",
                "        self.banned_ids = []\n",
                "        # Find all tokens in the entire vocabulary that contain 'e' or 'E'\n",
                "        for token_str, token_id in tokenizer.get_vocab().items():\n",
                "            if 'e' in token_str or 'E' in token_str:\n",
                "                self.banned_ids.append(token_id)\n",
                "\n",
                "    # This function intercept the logits right before Softmax inside model.generate()\n",
                "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
                "        for token_id in self.banned_ids:\n",
                "            scores[:, token_id] = -float('Inf')\n",
                "        return scores\n",
                "\n",
                "# Initialize our custom processor\n",
                "processor_list = LogitsProcessorList([NoVowelsProcessor(tokenizer)])\n",
                "\n",
                "test_prompt = \"The secret meaning of life is\"\n",
                "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
                "\n",
                "print(f\"Prompt: {test_prompt}\")\n",
                "print(\"Generating (Banning the letter 'e'):\")\n",
                "\n",
                "# Generate normally, but pass our custom logic in!\n",
                "out = model.generate(\n",
                "    **inputs, \n",
                "    max_new_tokens=20, \n",
                "    logits_processor=processor_list, # <--- The magic happens here\n",
                "    pad_token_id=tokenizer.eos_token_id,\n",
                "    do_sample=False\n",
                ")\n",
                "\n",
                "print(tokenizer.decode(out[0]))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c8bca42",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "081192f4",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "b74a12d8",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
